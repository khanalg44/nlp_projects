{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling and Similarities\n",
    "\n",
    "* [Latent Dirichlet Allocation (LDA) with Python](https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, re\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "import glob, sys\n",
    "\n",
    "data_dir = './datasets/bbc_sports/'\n",
    "\n",
    "from nlp_utils import prepare_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, Preprocess and Tokenize the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping the unreadable file:  ./datasets/bbc_sports/199.txt\n",
      "Total # of documents: 510\n"
     ]
    }
   ],
   "source": [
    "def read_data_files(data_dir):\n",
    "    all_data_files = glob.glob(data_dir+'*')   \n",
    "    raw_doc = []\n",
    "    for file in all_data_files:\n",
    "        # Use try and except method as some files may not be readable\n",
    "        try:\n",
    "            f = open(file, 'r', encoding='utf-8')\n",
    "            raw_doc.append(f.read())\n",
    "        except:\n",
    "            print (f\"skipping the unreadable file:  {file}\")\n",
    "            pass\n",
    "    return raw_doc\n",
    "\n",
    "raw_doc = read_data_files(data_dir)\n",
    "print (f\"Total # of documents: {len(raw_doc)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(raw_doc):\n",
    "    tokenized_doc = [prepare_text(doc, TOKENIZE=True, STEM=True) for doc in raw_doc]\n",
    "    dictionary = gensim.corpora.Dictionary(tokenized_doc)\n",
    "    numerical_corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
    "    return (dictionary, tokenized_doc, numerical_corpus)\n",
    "\n",
    "def model_lda(numerical_corpus, dictionary):\n",
    "    return gensim.models.LdaModel(corpus=numerical_corpus, num_topics=10, id2word=dictionary)\n",
    "\n",
    "def model_tfidf(numerical_corpus, dictionary):\n",
    "    tf_idf = gensim.models.TfidfModel(numerical_corpus, id2word=dictionary)\n",
    "    similarity_object = gensim.similarities.Similarity(data_dir,\n",
    "                                                       tf_idf[numerical_corpus],\n",
    "                                                       num_features=len(dictionary))\n",
    "    return tf_idf, similarity_object\n",
    "\n",
    "def model_w2v(tokenized_doc):\n",
    "    w2v_model = gensim.models.Word2Vec(\n",
    "                sentences=tokenized_doc,\n",
    "                size=300, # The size of the dense vector to represent each token or word \n",
    "                window=10, # The maximum distance between the target word and its neighboring word. \n",
    "                min_count=5, # Minimium frequency count of words. The model would ignore words with counts< min_count \n",
    "                workers=10) # How many threads to use behind the scenes\n",
    "\n",
    "    w2v_model.train(tokenized_doc, total_examples=len(tokenized_doc), epochs=15) \n",
    "    return w2v_model\n",
    "\n",
    "def save_model(model, model_name):\n",
    "        \n",
    "    import joblib\n",
    "    joblib.dump(model, model_name)\n",
    "    #model can be loaded as \n",
    "    #joblib.load(model_file_joblib)\n",
    "    print (f\"model: {model} is saved to {model_name}\")\n",
    "    \n",
    "    #import pickle\n",
    "    #model_file_pickle = './datasets/tfidf_model.p'\n",
    "    #pickle.dump(model, open(model_name, 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: LdaModel(num_terms=10321, num_topics=10, decay=0.5, chunksize=2000) is saved to ./datasets/lda_model\n",
      "\n",
      " first 10 most representative topics\n",
      "\n",
      "Topic #0 : 0.011*\"said\" + 0.008*\"play\" + 0.007*\"player\" + 0.006*\"would\" + 0.006*\"win\"\n",
      "\n",
      "Topic #1 : 0.011*\"said\" + 0.006*\"play\" + 0.005*\"win\" + 0.005*\"player\" + 0.005*\"game\"\n",
      "\n",
      "Topic #2 : 0.008*\"england\" + 0.006*\"game\" + 0.005*\"last\" + 0.005*\"said\" + 0.005*\"player\"\n",
      "\n",
      "Topic #3 : 0.008*\"said\" + 0.006*\"win\" + 0.005*\"player\" + 0.005*\"game\" + 0.005*\"year\"\n",
      "\n",
      "Topic #4 : 0.007*\"said\" + 0.006*\"play\" + 0.005*\"player\" + 0.005*\"win\" + 0.005*\"first\"\n",
      "\n",
      "Topic #5 : 0.009*\"said\" + 0.005*\"win\" + 0.005*\"play\" + 0.005*\"game\" + 0.005*\"england\"\n",
      "\n",
      "Topic #6 : 0.010*\"said\" + 0.008*\"game\" + 0.007*\"play\" + 0.007*\"win\" + 0.005*\"would\"\n",
      "\n",
      "Topic #7 : 0.008*\"said\" + 0.006*\"win\" + 0.006*\"play\" + 0.006*\"time\" + 0.005*\"back\"\n",
      "\n",
      "Topic #8 : 0.010*\"said\" + 0.006*\"game\" + 0.005*\"year\" + 0.005*\"win\" + 0.005*\"team\"\n",
      "\n",
      "Topic #9 : 0.009*\"game\" + 0.008*\"said\" + 0.007*\"play\" + 0.005*\"win\" + 0.004*\"also\"\n",
      "['wife', 'plans', 'go', 'tonight']\n",
      "[(57, 1), (2074, 1), (5826, 1)]\n",
      "\n",
      " 0 (8, '0.010*\"said\" + 0.006*\"game\" + 0.005*\"year\" + 0.005*\"win\" + 0.005*\"team\" + 0.005*\"cup\" + 0.005*\"go\" + 0.004*\"first\" + 0.004*\"us\" + 0.004*\"back\"')\n",
      "\n",
      " 1 (4, '0.007*\"said\" + 0.006*\"play\" + 0.005*\"player\" + 0.005*\"win\" + 0.005*\"first\" + 0.005*\"wale\" + 0.004*\"game\" + 0.004*\"would\" + 0.004*\"world\" + 0.004*\"time\"')\n"
     ]
    }
   ],
   "source": [
    "def sort_list(A, key=0):\n",
    "    return sorted(A, reverse=True, key=lambda x: x[key] )\n",
    "\n",
    "def print_top_k(topics, all_topics, k=2):\n",
    "    topics_sorted = sort_list(topics, key=1)\n",
    "    for i, topics in enumerate(topics_sorted[:k]):\n",
    "        idx = topics[0]\n",
    "        print (\"\\n\",i, all_topics[idx])\n",
    "\n",
    "def test_lda(raw_doc):\n",
    "    (dictionary, tokenized_doc, numerical_corpus) = prepare_data(raw_doc)\n",
    "    model = model_lda(numerical_corpus, dictionary)\n",
    "    \n",
    "    model_name = './datasets/lda_model'\n",
    "    save_model(model, model_name)\n",
    "\n",
    "    all_topics = model.print_topics()\n",
    "\n",
    "    print( \"\\n first 10 most representative topics\")\n",
    "    for i in range(10):\n",
    "        print(f\"\\nTopic #{i} : {model.print_topic(i, 5 )}\")\n",
    "\n",
    "    doc_new = \"My wife plans to go out tonight.\"\n",
    "    doc_new_prepared = prepare_text(doc_new, TOKENIZE=True)\n",
    "    print ( doc_new_prepared )\n",
    "    doc_bow = dictionary.doc2bow(doc_new_prepared)\n",
    "    print (doc_bow)\n",
    "    \n",
    "    topics= model.get_document_topics( doc_bow )\n",
    "    top_k_topics = print_top_k(topics, all_topics, k=2)\n",
    "\n",
    "test_lda(raw_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: TfidfModel(num_docs=510, num_nnz=67925) is saved to ./datasets/tfidf_model\n",
      "\n",
      "first 10 tokens:\n",
      " ['robben', 'sidelin', 'broken', 'footchelsea', 'winger', 'arjen', 'robben', 'broken', 'two', 'metatars']\n",
      "\n",
      "first 10 bow:\n",
      " [(23, 3), (45, 1), (46, 1), (50, 1), (57, 1), (71, 1), (94, 1), (106, 1), (110, 1), (111, 1)]\n",
      "\n",
      "first 10 tfidf:\n",
      " [(23, 0.060779464016390256), (45, 0.07075027144828656), (46, 0.014605368204018868), (50, 0.028797575313044728), (57, 0.015421487398221299), (71, 0.0404114823434502), (94, 0.015008913499203016), (106, 0.004434193825346702), (110, 0.020478063934517673), (111, 0.026530003463721384)]\n",
      "\n",
      "first 10 similarity scores:\n",
      " [0.024998276, 0.020571105, 0.009893991, 0.016575314, 0.014618116, 0.013003329, 0.020246074, 0.011348683, 1.0000001, 0.022904249]\n",
      "\n",
      "max score 1.0000001192092896 and max score index 8\n",
      "\n",
      "score: 1.0000001192092896 index:8\n",
      "\n",
      "score: 0.2723110318183899 index:220\n",
      "\n",
      "score: 0.1732272207736969 index:113\n",
      "\n",
      "Pritining the docs with very similar similarity score\n",
      "\n",
      "\n",
      " Robben sidelined with broken foot\n",
      "\n",
      "Chelsea winger Arjen Robben has broken two metatarsal bones in hi\n",
      "\n",
      "\n",
      " Robben plays down European return\n",
      "\n",
      "Injured Chelsea winger Arjen Robben has insisted that he only has\n",
      "\n",
      "\n",
      " Robben plays down European return\n",
      "\n",
      "Injured Chelsea winger Arjen Robben has insisted that he only has\n"
     ]
    }
   ],
   "source": [
    "def test_tfidf(raw_doc):\n",
    "    (dictionary, tokenized_doc, numerical_corpus) = prepare_data(raw_doc)\n",
    "    tf_idf, similarity_object = model_tfidf(numerical_corpus, dictionary)  \n",
    "    \n",
    "    model_name = './datasets/tfidf_model'\n",
    "    save_model(tf_idf, model_name)\n",
    "    \n",
    "    #query_text\n",
    "    q_text = raw_doc[8]\n",
    "    q_text_processed = prepare_text(q_text, TOKENIZE=True, STEM=True)\n",
    "    print ( \"\\nfirst 10 tokens:\\n\",q_text_processed[:10])\n",
    "    q_text_bow = dictionary.doc2bow(q_text_processed)\n",
    "    print ( \"\\nfirst 10 bow:\\n\",q_text_bow[:10])\n",
    "    q_text_tfidf = tf_idf[q_text_bow]\n",
    "    print ( \"\\nfirst 10 tfidf:\\n\",q_text_tfidf[:10] )\n",
    "    similarity_scores=list(similarity_object[q_text_tfidf])\n",
    "    print ( \"\\nfirst 10 similarity scores:\\n\", similarity_scores[:10])\n",
    "    \n",
    "    max_score = max(similarity_scores)\n",
    "    max_score_index = similarity_scores.index(max_score)\n",
    "\n",
    "    print (f\"\\nmax score {max_score} and max score index {max_score_index}\")\n",
    "    \n",
    "    sorted_score = sorted(similarity_scores, reverse=True)\n",
    "\n",
    "    indices = []\n",
    "    for i in range(3):\n",
    "        score = sorted_score[i]\n",
    "        indx  = similarity_scores.index(score)\n",
    "        print ( f\"\\nscore: {score} index:{indx}\")\n",
    "        \n",
    "        indices.append(indx)\n",
    "\n",
    "    print (\"\\nPritining the docs with very similar similarity score\\n\")\n",
    "    print (\"\\n\", raw_doc[indices[0]][:100] )\n",
    "    print (\"\\n\\n\", raw_doc[indices[1]][:100] )\n",
    "    print (\"\\n\\n\", raw_doc[indices[1]][:100] )\n",
    "\n",
    "test_tfidf(raw_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: Word2Vec(vocab=2817, size=300, alpha=0.025) is saved to ./datasets/w2v_model\n",
      "\n",
      "There are 2817 words. First 10 words:\n",
      "['robinson', 'blast', 'coach', 'andi', 'insist', 'livid', 'side', 'deni', 'two', 'tri']\n",
      "\n",
      "vectors.shape:(2817, 300)\n",
      "\n",
      "top 3 similar words to defend are [('bobbi', 0.8894723057746887), ('munich', 0.8797556161880493), ('milan', 0.875665009021759)]\n",
      "\n",
      "dictionary index: 31\n",
      "\n",
      "sanity check for word defend index: 31^th item in the dictionary: defend\n",
      "\n",
      "Similarity score of ## Word2Vec Model## 'celtic' and 'everton' : 0.913867712020874\n",
      "\n",
      "Similarity score of ## Word2Vec Model## 'good' and 'bad' : 0.7736691236495972\n",
      "\n",
      "Similarity score of ## Word2Vec Model## 'good' and 'celtic' : 0.11286429315805435\n",
      "\n",
      "Similarity score of ## Word2Vec Model## 'good' and 'good' : 1.0\n",
      "\n",
      "Similarity score of ## Word2Vec Model## 'kid' and 'men' : 0.17335648834705353\n"
     ]
    }
   ],
   "source": [
    "def test_word2vec(raw_doc):\n",
    "    (dictionary, tokenized_doc, numerical_corpus) = prepare_data(raw_doc)\n",
    "    model = model_w2v(tokenized_doc)\n",
    "    \n",
    "    model_name = './datasets/w2v_model'\n",
    "    save_model(model, model_name)\n",
    "    \n",
    "    words = list(model.wv.vocab)\n",
    "    print (f\"\\nThere are {len(words)} words. First 10 words:\\n{words[:10]}\")\n",
    "    \n",
    "    vectors = np.array([model.wv[word] for word in words])\n",
    "    print (f\"\\nvectors.shape:{vectors.shape}\")\n",
    "    \n",
    "    my_word = 'defend'\n",
    "    sim_word = model.wv.most_similar(positive=my_word)\n",
    "    print (f\"\\ntop 3 similar words to {my_word} are {sim_word[:3]}\")\n",
    "\n",
    "    v=model.wv[my_word]\n",
    "    idx=dictionary.doc2idx([my_word])[0]\n",
    "    print (f\"\\ndictionary index: {idx}\")\n",
    "    print (f\"\\nsanity check for word {my_word} index: {idx}^th item in the dictionary: {dictionary[idx]}\")\n",
    "    \n",
    "    word_pairs = [[\"celtic\", \"everton\"],\n",
    "              [\"good\", \"bad\"],\n",
    "              [\"good\", \"celtic\"],\n",
    "              [\"good\", \"good\"],\n",
    "              [\"kid\", \"men\"] ]\n",
    "\n",
    "    for (w1, w2) in word_pairs:\n",
    "        simi_score = model.wv.similarity(w1=w1, w2=w2)\n",
    "        print (f\"\\nSimilarity score of ## Word2Vec Model## '{w1}' and '{w2}' : {simi_score}\")\n",
    "\n",
    "test_word2vec(raw_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
