{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization : Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/gshyam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', \"Andrew's\", 'text,', \"isn't\", 'it?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is Andrew's text, isn't it?\"\n",
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'Andrew', \"'s\", 'text', ',', 'is', \"n't\", 'it', '?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'Andrew', \"'\", 's', 'text', ',', 'isn', \"'\", 't', 'it', '?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming : Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feet', 'wolves', 'cats', 'talked']\n"
     ]
    }
   ],
   "source": [
    "text = \"feet wolves cats talked\"\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feet wolv cat talk'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\" \".join(stemmer.stem(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foot wolf cat talked'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.WordNetLemmatizer()\n",
    "\" \".join(stemmer.lemmatize(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf : Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:\n",
      "   (0, 0)\t0.7071067811865476\n",
      "  (0, 2)\t0.7071067811865476\n",
      "  (1, 3)\t0.5773502691896257\n",
      "  (1, 0)\t0.5773502691896257\n",
      "  (1, 2)\t0.5773502691896257\n",
      "  (2, 1)\t0.7071067811865476\n",
      "  (2, 3)\t0.7071067811865476\n",
      "  (3, 1)\t1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good movie</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   good movie      like     movie       not\n",
       "0    0.707107  0.000000  0.707107  0.000000\n",
       "1    0.577350  0.000000  0.577350  0.577350\n",
       "2    0.000000  0.707107  0.000000  0.707107\n",
       "3    0.000000  1.000000  0.000000  0.000000\n",
       "4    0.000000  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "texts = [ \"good movie\", \"not a good movie\", \"did not like\", \"i like it\", \"good one\" ]\n",
    "# using default tokenizer in TfidfVectorizer\n",
    "tfidf= TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "\n",
    "features = tfidf.fit_transform(texts)\n",
    "\n",
    "print ('features:\\n',features)\n",
    "pd.DataFrame( features.todense(), columns=tfidf.get_feature_names() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Syntax Trees (AST) : Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n",
      "dict_keys(['a', 'b'])\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "str_dict1 = \"{'a':1, 'b':2}\"; dict1 = ast.literal_eval(str_dict1)\n",
    "str_lst1 = \"[1,2,3,4]\"; lst1=ast.literal_eval(str_lst1)\n",
    "str_lst2 = \"['php', 'mysql']\"; lst2=ast.literal_eval(str_lst2)\n",
    "\n",
    "#Now we can treat lst1 and lst2 as regular lists and dict1 as python dictionary\n",
    "print (len(lst1), len(lst2) )\n",
    "print (dict1.keys() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counter : Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original string: abcdeabcdabcaba\n",
      "most_common: [('a', 5), ('b', 4), ('c', 3)]\n",
      "sorted: ['a', 'b', 'c', 'd', 'e']\n",
      "list elements with repetitions: aaaaabbbbcccdde\n",
      "\n",
      "original list: ['aaa', 'abc', 'def', 'aaa', 'abc', 'aaa', 'def', 'aa']\n",
      "most_common: [('aaa', 3), ('abc', 2), ('def', 2)]\n",
      "sorted: ['aa', 'aaa', 'abc', 'def']\n",
      "list elements with repetitions: aaaaaaaaaaaabcabcdefdef\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "strr= 'abcdeabcdabcaba'\n",
    "c = Counter(strr)  # count elements from a string\n",
    "\n",
    "print ('original string:', strr)\n",
    "print  ( 'most_common:', c.most_common(3) )\n",
    "print  ( 'sorted:', sorted(c) )\n",
    "print ( 'list elements with repetitions:', ''.join(sorted(c.elements())) )\n",
    "print ()\n",
    "\n",
    "lst=['aaa', 'abc', 'def', 'aaa', 'abc', 'aaa', 'def', 'aa']\n",
    "c = Counter(lst)\n",
    "print ('original list:', lst)\n",
    "print  ( 'most_common:', c.most_common(3) )\n",
    "print  ( 'sorted:', sorted(c) )\n",
    "print ( 'list elements with repetitions:', ''.join(sorted(c.elements())) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort : Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'o', 'i', 'e', 'a']\n",
      "['u', 'o', 'i', 'e', 'a']\n",
      "['u', 'o', 'i', 'e', 'a']\n"
     ]
    }
   ],
   "source": [
    "py_set = {'e', 'a', 'u', 'o', 'i'}\n",
    "print(sorted(py_set, reverse=True))\n",
    "\n",
    "py_dict = {'e': 1, 'a': 2, 'u': 3, 'o': 4, 'i': 5}\n",
    "print(sorted(py_dict, reverse=True))\n",
    "\n",
    "frozen_set = frozenset(('e', 'a', 'u', 'o', 'i'))\n",
    "print(sorted(frozen_set, reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted with first element : [(1, 3), (2, 2), (3, 4), (4, 1)]\n",
      "Sorted with second element: [(4, 1), (2, 2), (1, 3), (3, 4)]\n"
     ]
    }
   ],
   "source": [
    "def take_first(x): return x[0]\n",
    "def take_second(x): return x[1]\n",
    "\n",
    "# random list\n",
    "random = [(2, 2), (3, 4), (4, 1), (1, 3)]\n",
    "print('Sorted with first element :', sorted(random, key=take_first) )\n",
    "print('Sorted with second element:', sorted(random, key=take_second) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jimmy', 90, 22), ('Terence', 75, 12), ('David', 75, 20), ('Jak', 50, 17), ('Alison', 50, 18)]\n"
     ]
    }
   ],
   "source": [
    "# List elements: (Student's Name, Marks out of 100 , Age)\n",
    "participant_list = [ ('Alison', 50, 18), ('Terence', 75, 12), ('David', 75, 20), ('Jimmy', 90, 22),('Jak', 50, 17) ]\n",
    "def sorter(item):\n",
    "    # Since highest marks first, least error = most marks\n",
    "    error = 100 - item[1]\n",
    "    age = item[2]\n",
    "    return (error, age)\n",
    "\n",
    "sorted_list = sorted(participant_list, key=sorter)\n",
    "print(sorted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', 6), ('a', 5), ('c', 4)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_dict = {'a': 5, 'b': 1, 'c': 4, 'd': 6, 'e': 2}\n",
    "\n",
    "sorted(py_dict.items(), key=lambda x: x[1], reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparce matrix from SciPy : Example\n",
    "\n",
    "     csr_matrix: Compressed Sparse Row format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x6 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in List of Lists format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import lil_matrix\n",
    "A = csr_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])\n",
    "v = np.array([1, 0, -1])\n",
    "A.dot(v)\n",
    "\n",
    "AA = lil_matrix((5,6))\n",
    "AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse as sp_sparse\n",
    "from numpy.random import rand\n",
    "\n",
    "A = sp_sparse.lil_matrix((1000, 1000))\n",
    "A[0, :100] = rand(100)\n",
    "A[1, 100:200] = A[0, :100]\n",
    "A.setdiag(rand(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A.tocsr()\n",
    "b = rand(1000)\n",
    "x = sp_sparse.linalg.spsolve(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "**short for Term Frequency–Inverse Document Frequency**\n",
    "\n",
    "In the following example of a simple text\n",
    "\n",
    "A vocabulary of 8 words is learned from the documents and each word is assigned a unique integer index in the output vector.\n",
    "\n",
    "The inverse document frequencies are calculated for each word in the vocabulary, assigning the lowest score of 1.0 to the most frequently observed word: “the” at index 7.\n",
    "\n",
    "Finally, the first document is encoded as an 8-element sparse array and we can review the final scorings of each word with different values for “the“, “fox“, and “dog” from the other words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vectorizer.vocabulary_: {'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "\n",
      "vectorizer.idf_: [1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "\n",
      "vector.shape: (1, 8)\n",
      "\n",
      "vector.toarray: [[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "        \"The dog.\",\n",
    "        \"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print('\\nvectorizer.vocabulary_:',vectorizer.vocabulary_)\n",
    "print('\\nvectorizer.idf_:',vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print('\\nvector.shape:',vector.shape)\n",
    "print('\\nvector.toarray:',vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vectorizer.vocabulary_: {'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "\n",
      "vector.shape: (1, 8)\n",
      "\n",
      "vector.toarray: [[1 1 1 1 1 1 1 2]]\n",
      "\n",
      "type(vector): <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "vectorizer = CountVectorizer() # create the transform\n",
    "vectorizer.fit(text) # tokenize and build vocab\n",
    "\n",
    "print('\\nvectorizer.vocabulary_:',vectorizer.vocabulary_) # summarize\n",
    "vector = vectorizer.transform(text) # encode document\n",
    "\n",
    "print('\\nvector.shape:',vector.shape)\n",
    "print('\\nvector.toarray:',vector.toarray())\n",
    "print('\\ntype(vector):', type(vector))\n",
    "X = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print (vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0]\n",
      " [0 0 1]]\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "print ( mlb.fit_transform([(1, 2), (3,)]) )\n",
    "#array([[1, 1, 0],\n",
    "#       [0, 0, 1]])\n",
    "print ( mlb.classes_ )\n",
    "#array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comedy', 'sci-fi', 'thriller']\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit_transform([{'sci-fi', 'thriller'}, {'comedy'}])\n",
    "#array([[0, 1, 1],\n",
    "#       [1, 0, 0]])\n",
    "print ( list(mlb.classes_) )\n",
    "#['comedy', 'sci-fi', 'thriller']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gshyam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "    text: a string\n",
    "    return: modified initial string\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ',text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join([word for word in text.split() if word not in STOPWORDS]) # delete stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming text to a vector\n",
    "\n",
    "Machine Learning algorithms work with numeric data and we cannot use the provided text data \"as is\". There are many ways to transform text data to numeric vectors. In this task you will try to use two of them.\n",
    "\n",
    "#### Bag of words\n",
    "\n",
    "One of the well-known approaches is a *bag-of-words* representation. To create this transformation, follow the steps:\n",
    "1. Find *N* most popular words in train corpus and numerate them. Now we have a dictionary of the most popular words.\n",
    "2. For each title in the corpora create a zero vector with the dimension equals to *N*.\n",
    "3. For each text in the corpora iterate over words which are in the dictionary and increase by 1 the corresponding coordinate.\n",
    "\n",
    "Let's try to do it for a toy example. Imagine that we have *N* = 4 and the list of the most popular words is \n",
    "\n",
    "    ['hi', 'you', 'me', 'are']\n",
    "\n",
    "Then we need to numerate them, for example, like this: \n",
    "\n",
    "    {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    "\n",
    "And we have the text, which we want to transform to the vector:\n",
    "\n",
    "    'hi how are you'\n",
    "\n",
    "For this text we create a corresponding zero vector \n",
    "\n",
    "    [0, 0, 0, 0]\n",
    "    \n",
    "And iterate over all words, and if the word is in the dictionary, we increase the value of the corresponding position in the vector:\n",
    "\n",
    "    'hi':  [1, 0, 0, 0]\n",
    "    'how': [1, 0, 0, 0] # word 'how' is not in our dictionary\n",
    "    'are': [1, 0, 0, 1]\n",
    "    'you': [1, 1, 0, 1]\n",
    "\n",
    "The resulting vector will be \n",
    "\n",
    "    [1, 1, 0, 1]\n",
    "   \n",
    "Implement the described encoding in the function *my_bag_of_words* with the size of the dictionary equals to 5000. To find the most common words use train data. You can test your code using the function *test_my_bag_of_words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "    result_vec = np.zeros(dict_size)\n",
    "    for word in text.split():\n",
    "        if word in words_to_index:\n",
    "            result_vec[words_to_index[word]] +=1\n",
    "    return result_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtained vector: [1. 1. 0. 1.]\n",
      "correct ansswer: [1, 1, 0, 1]\n",
      "The two are equal (T/F): True\n"
     ]
    }
   ],
   "source": [
    "# test my bag of words\n",
    "mytext = ['hi how are you']\n",
    "words_to_index = {'hi': 0, 'you': 1, 'me': 2, 'are': 3} # these are the most common words already found\n",
    "ans = [1, 1, 0, 1]\n",
    "\n",
    "for i, text in enumerate(mytext):\n",
    "    vec = my_bag_of_words(text, words_to_index, 4)\n",
    "    print ('obtained vector:', vec)\n",
    "    print ('correct ansswer:', ans)\n",
    "    print ( 'The two are equal (T/F):',(vec==ans).any() )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
