{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling and Similarities\n",
    "\n",
    "* [Latent Dirichlet Allocation (LDA) with Python](https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is a process of automatically identifying the topics present in a text corpus, it derives the hidden patterns among the words in the corpus in an unsupervised manner. Topics are defined as “a repeating pattern of co-occurring terms in a corpus”. Topic modelling can be described as a method for finding a group of words (i.e topic) from a collection of documents that best represents the information in the collection.\n",
    "\n",
    "As the name suggests, it is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Thus, assisting better decision making. \n",
    "\n",
    "Topic Modelling is different from rule-based text mining approaches that use regular expressions or dictionary based keyword searching techniques. It is an unsupervised approach used for finding and observing the bunch of words (called “topics”) in large clusters of texts.\n",
    "\n",
    "A good topic model results in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.\n",
    "\n",
    "Topic Models are very useful for the purpose for document clustering, organizing large blocks of textual data, information retrieval from unstructured text and feature selection. For Example – New York Times are using topic models to boost their user – article recommendation engines. Various professionals are using topic models for recruitment industries where they aim to extract latent features of job descriptions and map them to right candidates. They are being used to organize large datasets of emails, customer reviews, and user social media profiles.\n",
    "\n",
    "There are many approaches for obtaining topics from a text such as – Term Frequency and Inverse Document Frequency (TfIdf). NonNegative Matrix Factorization techniques. Latent Dirichlet Allocation(LDA) is the most popular topic modeling technique and in this article, we will discuss the same.\n",
    "\n",
    "LDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, re\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "utils_dir = '/Users/gshyam/utils/'\n",
    "data_dir = './datassets/bbc_sports/'\n",
    "\n",
    "sys.path.append(utils_dir)\n",
    "\n",
    "from nlp_utils import prepare_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see how it works with the following sentences.\n",
    "\n",
    "doc1 = \"I have big exam tomorrow and I need to study hard to get a good grade. This exam is harder than most of the exams.\"\n",
    "doc2 = \"My wife likes to go out with me but I prefer staying at home and studying.\"\n",
    "doc3 = \"Kids are playing football in the field and they seem to have fun\"\n",
    "doc4 = \"Sometimes I feel depressed while driving and it's hard to focus on the road.\"\n",
    "doc5 = \"I usually prefer reading at home but my wife prefers watching a TV.\"\n",
    "\n",
    "# array of documents aka corpus\n",
    "corpus = [doc1, doc2, doc3, doc4, doc5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and Tokenizing the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have big exam tomorrow and I need to study hard to get a good grade. This exam is harder than most of the exams.\n",
      "['big', 'exam', 'tomorrow', 'need', 'study', 'hard', 'get', 'good', 'grade', 'exam', 'harder', 'exams']\n",
      "['big', 'exam', 'tomorrow', 'need', 'studi', 'hard', 'get', 'good', 'grade', 'exam', 'harder', 'exam']\n"
     ]
    }
   ],
   "source": [
    "doc_tokenized = prepare_text(doc1, TOKENIZE=True)\n",
    "doc_stemmed = prepare_text(doc1, STEM=True)\n",
    "\n",
    "print (doc1)\n",
    "print (doc_tokenized)\n",
    "print (doc_stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['big',\n",
       "  'exam',\n",
       "  'tomorrow',\n",
       "  'need',\n",
       "  'study',\n",
       "  'hard',\n",
       "  'get',\n",
       "  'good',\n",
       "  'grade',\n",
       "  'exam',\n",
       "  'harder',\n",
       "  'exams'],\n",
       " ['wife', 'likes', 'go', 'prefer', 'staying', 'home', 'studying'],\n",
       " ['kids', 'playing', 'football', 'field', 'seem', 'fun'],\n",
       " ['sometimes', 'feel', 'depressed', 'driving', 'hard', 'focus', 'road'],\n",
       " ['usually', 'prefer', 'reading', 'home', 'wife', 'prefers', 'watching', 'tv']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = [prepare_text(doc, TOKENIZE=True) for doc in corpus]\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 items in the dictionary: key is index and value are the words\n",
      "(0, 'big')\n",
      "(1, 'exam')\n",
      "(2, 'exams')\n",
      "(3, 'get')\n",
      "(4, 'good')\n",
      "(5, 'grade')\n",
      "(6, 'hard')\n",
      "(7, 'harder')\n",
      "(8, 'need')\n",
      "(9, 'study')\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(tokenized_data)\n",
    "\n",
    "print (\"First 10 items in the dictionary: key is index and value are the words\")\n",
    "for item in list(dictionary.items())[:10]:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW) method \n",
    "this is a common and very popular method to convert a document in text form into numerical values which can be fed into a model. In this method each unque word in the doc is assignmed a label and the number of times a word appears in the doc is also assigned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the collection of texts to a numerical form\n",
    "numerical_corpus = [dictionary.doc2bow(text) for text in tokenized_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 2),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1)],\n",
       " [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)],\n",
       " [(18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)],\n",
       " [(6, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)],\n",
       " [(12, 1), (14, 1), (17, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the first line that second entry `(1,2)` represents word `exam` with index 1 and it appears 2 times in the doc. Similarly the word `hard` appears twice. hence we have `(5,2)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model\n",
    "\n",
    "The LDA model discovers the different topics that the documents represent and how much of each topic is present in a document. \n",
    "\n",
    "Python provides many great libraries for text mining practices, “gensim” is one such clean and beautiful library to handle text data. It is scalable, robust and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0 : 0.029*\"prefer\" + 0.029*\"hard\" + 0.029*\"kids\" + 0.029*\"focus\" + 0.029*\"wife\"\n",
      "Topic #1 : 0.029*\"wife\" + 0.029*\"home\" + 0.029*\"prefer\" + 0.029*\"playing\" + 0.029*\"hard\"\n",
      "Topic #2 : 0.114*\"wife\" + 0.114*\"home\" + 0.114*\"prefer\" + 0.059*\"reading\" + 0.059*\"watching\"\n",
      "Topic #3 : 0.135*\"exam\" + 0.071*\"hard\" + 0.071*\"big\" + 0.071*\"study\" + 0.071*\"tomorrow\"\n",
      "Topic #4 : 0.029*\"home\" + 0.029*\"prefer\" + 0.029*\"exam\" + 0.029*\"playing\" + 0.029*\"wife\"\n",
      "Topic #5 : 0.029*\"hard\" + 0.029*\"home\" + 0.029*\"prefer\" + 0.029*\"fun\" + 0.029*\"playing\"\n",
      "Topic #6 : 0.029*\"prefer\" + 0.029*\"playing\" + 0.029*\"hard\" + 0.029*\"wife\" + 0.029*\"home\"\n",
      "Topic #7 : 0.029*\"prefer\" + 0.029*\"home\" + 0.029*\"playing\" + 0.029*\"hard\" + 0.029*\"wife\"\n",
      "Topic #8 : 0.116*\"field\" + 0.116*\"seem\" + 0.116*\"kids\" + 0.116*\"fun\" + 0.116*\"football\"\n",
      "Topic #9 : 0.105*\"feel\" + 0.105*\"depressed\" + 0.105*\"sometimes\" + 0.105*\"driving\" + 0.105*\"hard\"\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.LdaModel(corpus=numerical_corpus, num_topics=10, id2word=dictionary)\n",
    "\n",
    "all_topics = model.print_topics()\n",
    "\n",
    "for i in range(10):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(f\"Topic #{i} : {model.print_topic(i, 5 )}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we trained and built our LDA model over the five simple sentences, whenever we want to detect the topic of a new sentence or text, we'll at first prepare the text and then push that into our model to get a topic. Let's try to predict a topic for a new sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "\n",
    "Let's find out a topic for a new doc using the previously trained model. \n",
    "`My wife plans to go out tonight.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wife', 'plans', 'go', 'tonight']\n",
      "[(11, 1), (17, 1)]\n"
     ]
    }
   ],
   "source": [
    "doc_new = \"My wife plans to go out tonight.\"\n",
    "doc_new_prepared = prepare_text(doc_new, TOKENIZE=True)\n",
    "print ( doc_new_prepared )\n",
    "doc_bow = dictionary.doc2bow(doc_new_prepared)\n",
    "print (doc_bow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here since our dictionary is not large enough the bag of words for the new doc has missed a couple of words `plans` and `tonight`. As only the words `wife: index=15` and `go : index=9` exists in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orinal list :\t\t[(2, 1), (3, 4), (4, 1), (1, 3)] \n",
      "sort with first:\t[(4, 1), (3, 4), (2, 1), (1, 3)] \n",
      "Sorted with second:\t[(3, 4), (1, 3), (2, 1), (4, 1)]\n"
     ]
    }
   ],
   "source": [
    "def sort_list(A, key=0):\n",
    "    # sort a list taking the take element of each item\n",
    "    # reverse=True to make the first element the largest\n",
    "    return sorted(A, reverse=True, key=lambda x: x[key] )\n",
    "\n",
    "A = [(2, 1), (3, 4), (4, 1), (1, 3)]\n",
    "A0=sort_list(A, 0)\n",
    "A1=sort_list(A, 1)\n",
    "print (f\"orinal list :\\t\\t{A} \\nsort with first:\\t{A0} \\nSorted with second:\\t{A1}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def print_topics(topics_sorted, all_topics, k=2):\n",
    "def print_top_k(topics, all_topics, k=2):\n",
    "    topics_sorted = sort_list(topics, key=1)\n",
    "    for i, topics in enumerate(topics_sorted[:k]):\n",
    "        idx = topics[0]\n",
    "        print (i, all_topics[idx])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (2, '0.114*\"wife\" + 0.114*\"home\" + 0.114*\"prefer\" + 0.059*\"reading\" + 0.059*\"watching\" + 0.059*\"prefers\" + 0.059*\"tv\" + 0.059*\"go\" + 0.059*\"likes\" + 0.059*\"staying\"')\n",
      "1 (0, '0.029*\"prefer\" + 0.029*\"hard\" + 0.029*\"kids\" + 0.029*\"focus\" + 0.029*\"wife\" + 0.029*\"playing\" + 0.029*\"home\" + 0.029*\"road\" + 0.029*\"fun\" + 0.029*\"exam\"')\n"
     ]
    }
   ],
   "source": [
    "topics= model.get_document_topics( doc_bow )\n",
    "top_k_topics = print_top_k(topics, all_topics, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top predictions for the new sentence `My wife plans to go out tonight.` are printed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity between documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['going', 'play', 'soccer', 'kids']\n",
      "[(21, 1)]\n",
      "[0.08770609 0.11107764 0.9765236  0.11107766 0.10820496]\n"
     ]
    }
   ],
   "source": [
    "lda_index = gensim.similarities.MatrixSimilarity(model[numerical_corpus])\n",
    "\n",
    "doc_new = \"We are going play soccer with the kids\"\n",
    "doc_new_prepared = prepare_text(doc_new, TOKENIZE=True)\n",
    "print ( doc_new_prepared )\n",
    "doc_bow = dictionary.doc2bow(doc_new_prepared)\n",
    "print (doc_bow)\n",
    "\n",
    "similarities = lda_index[model[doc_bow]]\n",
    "\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means this new sentence is closest in the meaning to `doc3`  with probability `0.976`. And it makes sense that the new doc `We are going play soccer with the kids` is closest in meaning to `Kids are playing football in the field and they seem to have fun`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things that can be added here\n",
    "\n",
    "* N grams vocabulary\n",
    "* Word Embeddings where `play` and `playing` mean the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Similarity with BBC sport data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, Preprocess and Tokenize the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, sys\n",
    "\n",
    "data_dir = './datasets/bbc_sports/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping the unreadable file:  ./datasets/bbc_sports/199.txt\n",
      "Total # of documents: 510\n"
     ]
    }
   ],
   "source": [
    "# Read all the files and store it in a doc\n",
    "\n",
    "def read_data_files(data_dir):\n",
    "    all_data_files = glob.glob(data_dir+'*')   \n",
    "    raw_doc = []\n",
    "    for file in all_data_files:\n",
    "        # Use try and except method as some files may not be readable\n",
    "        try:\n",
    "            f = open(file, 'r', encoding='utf-8')\n",
    "            raw_doc.append(f.read())\n",
    "        except:\n",
    "            print (f\"skipping the unreadable file:  {file}\")\n",
    "            pass\n",
    "    return raw_doc\n",
    "\n",
    "raw_doc = read_data_files(data_dir)\n",
    "print (f\"Total # of documents: {len(raw_doc)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 150 character (before processing) in first doc: \n",
      "\n",
      "Fuming Robinson blasts officials\n",
      "\n",
      "England coach Andy Robinson insisted he was \"livid\" after his side were denied two tries in Sunday's 19-13 Six Natio\n"
     ]
    }
   ],
   "source": [
    "print (f\"The first 150 character (before processing) in first doc: \\n\\n{raw_doc[0][:150]}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 50 tokens (words) (after processing) in first doc: \n",
      "\n",
      "['fume', 'robinson', 'blast', 'officialsengland', 'coach', 'andi', 'robinson', 'insist', 'livid', 'side', 'deni', 'two', 'tri', 'sunday', '1913', 'six', 'nation', 'loss', 'ireland', 'dublinmark']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenized_doc = [prepare_text(doc, TOKENIZE=True, STEM=True) for doc in raw_doc]\n",
    "\n",
    "print (f\"The first 50 tokens (words) (after processing) in first doc: \\n\\n{tokenized_doc[0][:20]}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10321 items in the dictionary\n",
      "First 10 items in the dictionary: key is index and value are the words\n",
      "(0, '1913')\n",
      "(1, 'abl')\n",
      "(2, 'absolut')\n",
      "(3, 'african')\n",
      "(4, 'ahead')\n",
      "(5, 'andi')\n",
      "(6, 'awesom')\n",
      "(7, 'back')\n",
      "(8, 'ball')\n",
      "(9, 'bbc')\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(tokenized_doc)\n",
    "print (f\"There are {len(list(dictionary.items()))} items in the dictionary\")\n",
    "print (\"First 10 items in the dictionary: key is index and value are the words\")\n",
    "for item in list(dictionary.items())[:10]:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "\n",
    "Create a numerical corpus. A corpus is a list of bags of words. A bag-of-words representation for a document just lists the number of times each word occurs in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 2), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 3), (24, 1), (25, 1), (26, 1), (27, 1), (28, 4), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 2), (38, 1), (39, 1), (40, 1), (41, 1), (42, 2), (43, 2), (44, 2), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 4), (54, 1), (55, 1), (56, 1), (57, 2), (58, 1), (59, 1), (60, 3), (61, 2), (62, 1), (63, 1), (64, 2), (65, 1), (66, 2), (67, 1), (68, 1), (69, 3), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 3), (76, 1), (77, 1), (78, 2), (79, 2), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 3), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 2), (98, 1), (99, 1), (100, 1), (101, 4), (102, 1), (103, 5), (104, 1), (105, 2), (106, 1), (107, 1), (108, 1), (109, 1), (110, 2), (111, 1), (112, 1), (113, 4), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 2), (123, 1), (124, 1), (125, 1), (126, 2), (127, 1), (128, 1), (129, 1), (130, 1), (131, 2), (132, 7), (133, 1), (134, 2), (135, 1), (136, 2), (137, 1), (138, 1), (139, 1), (140, 1), (141, 2), (142, 3), (143, 1), (144, 1), (145, 1), (146, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Transform the collection of texts to a numerical form\n",
    "numerical_corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
    "print ( numerical_corpus[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now we will create a similarity measure object in tf-idf space. tf-idf stands for term frequency-inverse document frequency. Term frequency is how often the word shows up in the document and inverse document fequency scales the value by how rare the word is in the corpus.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=510, num_nnz=67925)\n"
     ]
    }
   ],
   "source": [
    "tf_idf = gensim.models.TfidfModel(numerical_corpus, id2word=dictionary)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 510 documents in 0 shards (stored under ./datasets/)\n",
      "<class 'gensim.similarities.docsim.Similarity'>\n"
     ]
    }
   ],
   "source": [
    "similarity_object = gensim.similarities.Similarity('./datasets/', tf_idf[numerical_corpus], num_features=len(dictionary))\n",
    "print(similarity_object)\n",
    "print(type(similarity_object))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./datasets/tfidf_model']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dump the model object to a file\n",
    "# can use pickle as well\n",
    "# but joblib is faster for large number of numpy arrays\n",
    "\n",
    "import joblib\n",
    "model_file_joblib = './datasets/tfidf_model'\n",
    "joblib.dump(similarity_object, model_file_joblib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_file_pickle = './datasets/tfidf_model.p'\n",
    "pickle.dump(similarity_object, open(model_file_pickle, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and using the  stored model\n",
    "\n",
    "Now create a query document and convert it to tf-idf.\n",
    "\n",
    "query document is the one that we want to find the similar documents accordingly\n",
    "\n",
    "we're going to use `raw_doc[8]` as our query doc and will try to see if our model would be able to find the same document as the most similar one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "similarity_object = joblib.load(model_file_joblib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 tokens:\n",
      " ['robben', 'sidelin', 'broken', 'footchelsea', 'winger', 'arjen', 'robben', 'broken', 'two', 'metatars']\n",
      "first 10 bow:\n",
      " [(23, 3), (45, 1), (46, 1), (50, 1), (57, 1), (71, 1), (94, 1), (106, 1), (110, 1), (111, 1)]\n"
     ]
    }
   ],
   "source": [
    "#query_text\n",
    "q_text = raw_doc[8]\n",
    "q_text_processed = prepare_text(q_text, TOKENIZE=True, STEM=True)\n",
    "print ( \"first 10 tokens:\\n\",q_text_processed[:10])\n",
    "q_text_bow = dictionary.doc2bow(q_text_processed)\n",
    "print ( \"first 10 bow:\\n\",q_text_bow[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 tfidf:\n",
      " [(23, 0.060779464016390256), (45, 0.07075027144828656), (46, 0.014605368204018868), (50, 0.028797575313044728), (57, 0.015421487398221299), (71, 0.0404114823434502), (94, 0.015008913499203016), (106, 0.004434193825346702), (110, 0.020478063934517673), (111, 0.026530003463721384)]\n"
     ]
    }
   ],
   "source": [
    "q_text_tfidf = tf_idf[q_text_bow]\n",
    "print ( \"first 10 tfidf:\\n\",q_text_tfidf[:10] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 similarity scores:\n",
      " [0.024998276, 0.020571105, 0.009893991, 0.016575314, 0.014618116, 0.013003329, 0.020246074, 0.011348683, 1.0000001, 0.022904249]\n"
     ]
    }
   ],
   "source": [
    "similarity_scores=list(similarity_object[q_text_tfidf])\n",
    "print ( \"first 10 similarity scores:\\n\", similarity_scores[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max score 1.0000001192092896 and max score index 8\n"
     ]
    }
   ],
   "source": [
    "max_score = max(similarity_scores)\n",
    "max_score_index = similarity_scores.index(max_score)\n",
    "\n",
    "print (f\"max score {max_score} and max score index {max_score_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the `similarity score` for 8th doc is 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 1.0000001192092896 index:8\n",
      "score: 0.2723110318183899 index:220\n",
      "score: 0.1732272207736969 index:113\n"
     ]
    }
   ],
   "source": [
    "sorted_score = sorted(similarity_scores, reverse=True)\n",
    "\n",
    "for i in range(3):\n",
    "    score = sorted_score[i]\n",
    "    indx = similarity_scores.index(score)\n",
    "    print ( f\"score: {score} index:{indx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Robben sidelined with broken foot\\n\\nChelsea winger Arjen Robben has broken two metatarsal bones in his foot and will be out for at least six weeks.\\n\\nRobben had an MRI scan on the injury, sustained during the Premiership win at Blackburn, on Monday. \"Six weeks is the average time to heal this injury and then I need a few more weeks to be completely fit again,\" he told Dutch newspaper Algemeen Dagblad. \"I had a feeling it was serious but because of the swelling it was impossible to make a final diagnosis.\" The 21-year-old missed the first three months of the season with a similar injury after a challenge with Roma\\'s Olivier Dacourt. And he added: \"It felt different then last summer when I had the same injury on my other foot. \"Then I could walk already after three days but I stayed sidelined for a long period. I hope that it will now take me six to eight weeks.\" Chelsea physio Mike Banks was hopeful that Robben could return at some point in March. \"The fractures are tiny and he could be playing next month,\" Banks told the club\\'s website. \"One is a chip on the side of his foot, the other is a small break on the third metatarsal. \"But this is not the traditional metatarsal that has become so famous since the last World Cup and which has kept Scott Parker out for two months.\"\\n\\nDavid Beckham suffered a broken metatarsal in the build up to the 2002 World Cup in Korea and Japan. Robben, who has been a key part of the Blues\\' push for four trophies, claims he knew instantly something was wrong when he was felled by Blackburn midfielder Aaron Mokoena. \"I felt my leg go,\" he said. \"I felt it straight away after Mokoena hit me with a wild kick on my left foot.\"\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_doc[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Robben plays down European return\\n\\nInjured Chelsea winger Arjen Robben has insisted that he only has a 10% chance of making a return against Barcelona in the Champions League.\\n\\nThe 21-year-old has been sidelined since breaking a foot against Blackburn last month. Chelsea face Barcelona at home on 8 March having lost 2-1 in the first leg. And Robben told the Daily Star: \"It is not impossible that I will play against Barcelona but it is just a very, very small chance - about 10%.\"\\n\\nRobben has been an inspirational player for Chelsea this season following a switch from PSV Einhoven last summer. He added: \"My recovery is going better than we expected a few weeks ago but I think the Barcelona game will come too soon. \"I won\\'t take any risks and come back too soon.\"\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_doc[220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kenyon denies Robben Barca return\\n\\nChelsea chief executive Peter Kenyon has played down reports that Arjen Robben will return for the Champions League match against Barcelona.\\n\\n\"He\\'s been responding well to treatment and started running on Friday, but we\\'ll have to wait and see,\" he told BBC Five Live\\'s Sportsweek. \"We\\'re looking to getting him back as soon as possible, but he\\'ll be back when it\\'s right for him and for us. \"There\\'s no plans at the moment around the Barcelona game.\" His comments contradict those of chiropractor Jean Pierre Meersseman who treated the Dutchman after he fractured his foot at the start of February. Robben had been expected to be out for six weeks, but Meersseman hinted that the winger could be fit for the vital Stamford Bridge game on 8 March. \"I hope he can be back and I will try to help him make that happen,\" Meersseman told the Mail on Sunday. \"I put everything right with Arjen\\'s foot the last time I saw him 12 days ago. It was an obvious correction and easy to perform. \"I know he was pleased with what I did and now that he is running again. I am due to see him one more time again in the next few days.\" Meersseman is the medical co-ordinator at Italian side AC Milan.\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_doc[113]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping the unreadable file:  ./datasets/bbc_sports/199.txt\n"
     ]
    }
   ],
   "source": [
    "raw_doc = read_data_files(data_dir)\n",
    "tokenized_doc = [prepare_text(doc, TOKENIZE=True, STEM=True) for doc in raw_doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1177052, 1410330)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build vocabulary and train model\n",
    "# you can see what all the inner parameters mean from the official gensim documentation\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(\n",
    "            sentences=tokenized_doc,\n",
    "            size=300, # The size of the dense vector to represent each token or word \n",
    "            window=10, # The maximum distance between the target word and its neighboring word. \n",
    "            min_count=5, # Minimium frequency count of words. The model would ignore words that do not satisfy the min_count \n",
    "            workers=10) # How many threads to use behind the scenes\n",
    "\n",
    "w2v_model.train(tokenized_doc, total_examples=len(tokenized_doc), epochs=15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2817 words. First 10 words:\n",
      "['robinson', 'blast', 'coach', 'andi', 'insist', 'livid', 'side', 'deni', 'two', 'tri']\n"
     ]
    }
   ],
   "source": [
    "words = list(w2v_model.wv.vocab)\n",
    "print (f\"There are {len(words)} words. First 10 words:\\n{words[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors.shape:(2817, 300)\n"
     ]
    }
   ],
   "source": [
    "vectors = np.array([w2v_model.wv[word] for word in words])\n",
    "print (f\"vectors.shape:{vectors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my word: defend\n",
      "dictionary index: 31\n",
      "sanity check: 31^th item in the dictionary: defend\n"
     ]
    }
   ],
   "source": [
    "my_word = 'defend'\n",
    "print (f\"my word: {my_word}\")\n",
    "v=w2v_model.wv[my_word]\n",
    "idx=dictionary.doc2idx([my_word])[0]\n",
    "print ('dictionary index:', idx)\n",
    "print (f\"sanity check: {idx}^th item in the dictionary: {dictionary[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=\"celtic\"\n",
    "v = w2v_model.wv[word].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the vocabulary, we just need to call train(...) to start training the Word2Vec model. Behind the scenes, what’s happening here is that we are training a neural network with a single hidden layer where we train the model to predict the current word based on the context (using the default neural architecture). However, we are not going to use the neural network after training! Instead, the goal is to learn the weights of the hidden layer. These weights are essentially the word vectors that we’re trying to learn. The resulting learned vector is also known as the embeddings. You can think of these embeddings as some features that describe the target word. For example, the word king may be described by the gender, age, the type of people the king associates with, etc.\n",
    "\n",
    "Let's see similarity on some sport types. This first example shows a simple look up of words similar to the word ‘match’. All we need to do here is to call the most_similar function and provide the word ‘match’ as the positive example. This returns the top 10 similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lost', 0.909685492515564),\n",
       " ('defeat', 0.8464573621749878),\n",
       " ('seven', 0.8464318513870239),\n",
       " ('french', 0.8307112455368042),\n",
       " ('two', 0.8213168978691101),\n",
       " ('nine', 0.8158985376358032),\n",
       " ('arres', 0.805950403213501),\n",
       " ('huge', 0.7911980152130127),\n",
       " ('tournament', 0.7869153618812561),\n",
       " ('beaten', 0.7826323509216309)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word=\"match\"\n",
    "w2v_model.wv.most_similar(positive=word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crisi', 0.9595956802368164),\n",
       " ('physio', 0.9509631395339966),\n",
       " ('destroy', 0.9400373697280884),\n",
       " ('monday', 0.9334959983825684),\n",
       " ('bone', 0.9209457635879517),\n",
       " ('prefer', 0.9118180274963379),\n",
       " ('separ', 0.9071800112724304),\n",
       " ('doubt', 0.9070960283279419),\n",
       " ('thumb', 0.9057219624519348),\n",
       " ('session', 0.9042330980300903)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word=\"fractur\"\n",
    "w2v_model.wv.most_similar(positive=word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('got', 0.9682800769805908),\n",
       " ('everi', 0.9312726855278015),\n",
       " ('get', 0.9273092746734619),\n",
       " ('look', 0.9247929453849792),\n",
       " ('hard', 0.9181417226791382),\n",
       " ('difficult', 0.9140352010726929),\n",
       " ('might', 0.9113308191299438),\n",
       " ('coupl', 0.9102075099945068),\n",
       " ('like', 0.9067820310592651),\n",
       " ('mental', 0.9051308631896973)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets try the same with an adjective : \"good\"\n",
    "word=\"good\"\n",
    "\n",
    "w2v_model.wv.most_similar(positive=word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('newcastl', 0.916409969329834),\n",
       " ('everton', 0.9107216596603394),\n",
       " ('old', 0.9064370393753052),\n",
       " ('alex', 0.9049527645111084),\n",
       " ('midfield', 0.8896152973175049),\n",
       " ('red', 0.888375997543335),\n",
       " ('wayn', 0.8713171482086182),\n",
       " ('ferguson', 0.864258348941803),\n",
       " ('blue', 0.8639053106307983),\n",
       " ('bolton', 0.8627519607543945)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word=\"celtic\"\n",
    "w2v_model.wv.most_similar(positive=word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the results actually make sense. All of the related words tend to be used in similar contexts.\n",
    "\n",
    "Now you could even use Word2Vec to compute similarity between two words in the vocabulary by invoking the similarity(...) function and passing in the relevant words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score of celtic and everton : 0.9107215404510498\n",
      "Similarity score of good and bad : 0.7108150124549866\n",
      "Similarity score of good and celtic : 0.0958632230758667\n",
      "Similarity score of good and good : 1.0000001192092896\n",
      "Similarity score of kid and men : 0.23357325792312622\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [[\"celtic\", \"everton\"],\n",
    "              [\"good\", \"bad\"],\n",
    "              [\"good\", \"celtic\"],\n",
    "              [\"good\", \"good\"],\n",
    "              [\"kid\", \"men\"]\n",
    "             ]\n",
    "\n",
    "for (w1, w2) in word_pairs:\n",
    "    simi_score = w2v_model.wv.similarity(w1=w1, w2=w2)\n",
    "    print (f\"Similarity score of {w1} and {w2} : {simi_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the above four snippets compute the cosine similarity between the two specified words using word vectors (embeddings) of each. From the scores above, it makes sense that celtic is highly similar to nottingham but good is dissimilar to bad. If you do a similarity between two identical words, the score will be 1.0 as the range of the cosine similarity score will always be between [0.0-1.0]. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec model\n",
    "\n",
    "doc2vec model gets its algorithm from word2vec.\n",
    "\n",
    "In word2vec there is no need to label the words, because every word has their own semantic meaning in the vocabulary. But in case of doc2vec, there is a need to specify that how many number of words or sentences convey a semantic meaning, so that the algorithm could identify it as a single entity. For this reason, we are specifying labels or tags to sentence or paragraph depending on the level of semantic meaning conveyed.\n",
    "\n",
    "If we specify a single label to multiple sentences in a paragraph, it means that all the sentences in the paragraph are required to convey the meaning. On the other hand, if we specify variable labels to all the sentences in a paragraph, it means that each conveys a semantic meaning and they may or may not have similarity among them.\n",
    "\n",
    "In simple terms, a label means semantic meaning of something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
