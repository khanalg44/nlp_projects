{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next word prediction using LSTM\n",
    "\n",
    "- https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "- https://towardsdatascience.com/next-word-prediction-with-nlp-and-deep-learning-48b9fe0a17bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "import json \n",
    "import pickle \n",
    "\n",
    "# NLP imports\n",
    "import re\n",
    "import nltk, gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "data_dir = '../nlp_datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Full text.\n",
      "['project', 'gutenberg', 'ebook', 'metamorphosi', 'franz', 'kafka', 'translat', 'david', 'wylli', 'ebook']\n"
     ]
    }
   ],
   "source": [
    "def load_text_data():\n",
    "    file = open(data_dir + 'Metamorphosis_Franz_Kafka.txt', 'r', encoding = \"utf8\")\n",
    "    doc = ''\n",
    "    for line in file.readlines():\n",
    "        doc = doc + line\n",
    "    return doc\n",
    "\n",
    "def preprocess_text(raw_doc=None, return_raw_data=False):\n",
    "    \n",
    "    if raw_doc is None:\n",
    "        print (\"Loading the Full text.\")\n",
    "        raw_doc = load_text_data()\n",
    "    \n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\n]')\n",
    "    BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "    doc = raw_doc.lower()\n",
    "    doc = REPLACE_BY_SPACE_RE.sub(' ',doc)\n",
    "    doc = BAD_SYMBOLS_RE.sub('', doc)\n",
    "    doc = ' '.join([word for word in doc.split() if word not in STOPWORDS])\n",
    "    \n",
    "    # remove the different form of the same word\n",
    "    doc = doc.split(' ')\n",
    "    stemmer = PorterStemmer()\n",
    "    doc = [stemmer.stem(d) for d in doc]\n",
    "    \n",
    "    if return_raw_data:\n",
    "        return raw_doc, doc\n",
    "    return doc\n",
    "\n",
    "data = preprocess_text()\n",
    "\n",
    "print (doc_words[:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Full text.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Metamorphosis, by Franz Kafka\\nTranslated by David Wyllie.\\n\\nThis eBoo'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = preprocess_text(return_raw_data=True)[0]\n",
    "raw_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2252 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=10000, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\n', lower=True)\n",
    "tokenizer.fit_on_texts(data)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11], [51], [118], [439], [593], [594], [595], [506], [507], [118]]\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(data)\n",
    "print ( X[:10] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The most repeated words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 repeated words are:\n",
      "[('gregor', 298), ('would', 187), ('room', 133), ('could', 120), ('work', 114), ('even', 104), ('father', 102), ('sister', 101), ('door', 97), ('mother', 90)]\n"
     ]
    }
   ],
   "source": [
    "# Also check these out\n",
    "# tokenizer.index_docs, tokenizer.index_word\n",
    "# tokenizer.word_index, tokenizer.word_docs\n",
    "word_counts = tokenizer.word_counts  # same as json.loads(config['word_counts'])\n",
    "word_counts_sorted = sorted(word_counts.items(), key=lambda kv:kv[1], reverse=True)[:10]\n",
    "print (f\"Top 5 repeated words are:\\n{word_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2253\n"
     ]
    }
   ],
   "source": [
    "# Index for each words\n",
    "#for item in list(json.loads(config['word_index']).items())[:5]:\n",
    "#    print (item)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 51, 118, 439, 593]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences([data])[0]\n",
    "sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 words: ['project', 'gutenberg', 'ebook', 'metamorphosi', 'franz']\n",
      "(word, index): ('project', 11)\n",
      "(word, index): ('gutenberg', 51)\n",
      "(word, index): ('ebook', 118)\n",
      "(word, index): ('metamorphosi', 439)\n",
      "(word, index): ('franz', 593)\n"
     ]
    }
   ],
   "source": [
    "print (\"First 5 words:\", data[:5])\n",
    "for w in doc_words[:5]:\n",
    "    print (f'(word, index): {w, tokenizer.word_index[w]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11924, 16) (11924,)\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(doc, n_steps=16):\n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "    tokenizer.fit_on_texts([doc])\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    sequences = tokenizer.texts_to_sequences([doc_words])[0]\n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)-n_steps-1):\n",
    "        X.append(sequences[i:i+n_steps])#i+n_steps])\n",
    "        y.append(sequences[i+n_steps])\n",
    "    return (np.array(X), np.array(y), tokenizer, vocab_size)\n",
    "\n",
    "n_steps = 16\n",
    "(X, y, tokenizer, vocab_size ) = prepare_sequence(data, n_steps=n_steps)\n",
    "#y = to_categorical(y, num_classes=vocab_size)\n",
    "print ( X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (7154, 16) y_train.shape:(7154,)\n",
      "X_test.shape: (2385, 16) y_test.shape:(2385,)\n",
      "X_val.shape: (2385, 16) y_val.shape:(2385,)\n"
     ]
    }
   ],
   "source": [
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_, X_test, y_train_, y_test = train_test_split(X, y, test_size=0.2, random_state=8848)\n",
    "X_train,  X_val,  y_train , y_val = train_test_split(X_train_, y_train_,\n",
    "                                                     test_size=0.25, random_state=8848)\n",
    "\n",
    "print (f\"X_train.shape: {X_train.shape} y_train.shape:{y_train.shape}\" )\n",
    "print (f\"X_test.shape: {X_test.shape} y_test.shape:{y_test.shape}\" )\n",
    "print (f\"X_val.shape: {X_val.shape} y_val.shape:{y_val.shape}\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
