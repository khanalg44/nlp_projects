{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Project with StackOverflow posts\n",
    "\n",
    "In this task you will deal with a dataset of post titles from StackOverflow. You are provided a split to 3 sets: *train*, *validation* and *test*. All corpora (except for *test*) contain titles of the posts and corresponding tags (100 tags are available). The *test* set is provided for Coursera's grading and doesn't contain answers. Upload the corpora using *pandas* and look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import sparse as sp_sparse\n",
    "from numpy.random import rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "data_dir = \"../nlp_datasets/SE_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data = pd.read_csv(filename, sep='\\t')\n",
    "    data['tags'] = data['tags'].apply(literal_eval)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to draw a stacked dotplot in R?</td>\n",
       "      <td>[r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mysql select all records where a datetime fiel...</td>\n",
       "      <td>[php, mysql]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to terminate windows phone 8.1 app</td>\n",
       "      <td>[c#]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title          tags\n",
       "0                How to draw a stacked dotplot in R?           [r]\n",
       "1  mysql select all records where a datetime fiel...  [php, mysql]\n",
       "2             How to terminate windows phone 8.1 app          [c#]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = read_data(data_dir+'train.tsv')\n",
    "validation = read_data(data_dir+'validation.tsv')\n",
    "test = pd.read_csv(data_dir+'test.tsv', sep='\\t') # test doesn't have any tags\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training X shape: (100000,) val X.shape: (30000,) test X.shape: (20000,)\n",
      "training y shape: (100000,) val y.shape: (30000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = train['title'].values, train['tags'].values\n",
    "X_val, y_val = validation['title'].values, validation['tags'].values\n",
    "X_test = test['title'].values\n",
    "\n",
    "print ('training X shape:', X_train.shape,\n",
    "       'val X.shape:', X_val.shape,\n",
    "       'test X.shape:', X_test.shape)\n",
    "\n",
    "print ('training y shape:', y_train.shape,\n",
    "       'val y.shape:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gshyam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def text_prepare(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ',text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join([word for word in text.split() if word not in STOPWORDS]) # delete stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      " answer \t \t : sql server equivalent excels choose function \n",
      " correct answer \t : sql server equivalent excels choose function \n",
      " both are equal \t : True\n",
      "1\n",
      "\n",
      " answer \t \t : free c++ memory vectorint arr \n",
      " correct answer \t : free c++ memory vectorint arr \n",
      " both are equal \t : True\n"
     ]
    }
   ],
   "source": [
    "# test above function\n",
    "examples = [\"SQL Server - any equivalent of Excel's CHOOSE function?\",\n",
    "            \"How to free c++ memory vector<int> * arr?\"]\n",
    "answers = [\"sql server equivalent excels choose function\",\n",
    "           \"free c++ memory vectorint arr\"]\n",
    "for i in range(2):\n",
    "    text = examples[i]\n",
    "    ans = text_prepare(text)\n",
    "    ans_correct = answers[i]\n",
    "    print (i)\n",
    "    print ('\\n answer \\t \\t :', ans,\n",
    "           '\\n correct answer \\t :', ans_correct,\n",
    "           '\\n both are equal \\t :', ans==ans_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your data from train, test and validation set\n",
    "X_train = [text_prepare(x) for x in X_train]\n",
    "X_val = [text_prepare(x) for x in X_val]\n",
    "X_test = [text_prepare(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['draw stacked dotplot r', 'mysql select records datetime field less specified value']\n",
      "['warning mysql_query expects parameter 2 resource object given', 'get click coordinates input typeimage via javascript']\n",
      "['odbc_exec always fail', 'access base classes variable within child class']\n"
     ]
    }
   ],
   "source": [
    "print ( X_train[:2])\n",
    "print ( X_test[:2])\n",
    "print ( X_val[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['r']), list(['php', 'mysql'])], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordsTagsCount\n",
    "Find 3 most popular tags and 3 most popular words in the train data and submit the results to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Dictionary of all tags from train corpus with their counts.\n",
    "all_tags = [item for item_list in y_train for item in item_list]\n",
    "tags_counts = Counter(all_tags)\n",
    "\n",
    "# Dictionary of all words from train corpus with their counts.\n",
    "#all_words = [word for line in X_train for word in line.split()]\n",
    "ALL_WORDS = [word for line in X_train for word in line.split()]\n",
    "words_counts = Counter(ALL_WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 3 tags: [('javascript', 19078), ('c#', 19077), ('java', 18661)]\n",
      "The top 3 words: [('using', 8278), ('php', 5614), ('java', 5501)]\n"
     ]
    }
   ],
   "source": [
    "# The most common items in tags and words\n",
    "print ( 'The top 3 tags:', tags_counts.most_common(3) )\n",
    "print ( 'The top 3 words:', words_counts.most_common(3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most_common_tags [('javascript', 19078), ('c#', 19077), ('java', 18661)]\n",
      "most_common_words [('using', 8278), ('php', 5614), ('java', 5501)]\n"
     ]
    }
   ],
   "source": [
    "# get a sorted dictionary\n",
    "tags_counts_sorted  =  sorted(tags_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "words_counts_sorted = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "most_common_tags  = tags_counts_sorted[:3]\n",
    "most_common_words = words_counts_sorted[:3]\n",
    "\n",
    "print ('most_common_tags',most_common_tags)\n",
    "print ('most_common_words',most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_SIZE = 1000\n",
    "VOCAB = words_counts.most_common(DICT_SIZE)  # already sorted\n",
    "WORDS_TO_INDEX = {item[0]:ii for ii, item in enumerate(VOCAB) }\n",
    "#VOCAB is already sorted hence we don't need to do the following.\n",
    "#WORDS_TO_INDEX = {item[0]:ii for ii, item in enumerate( sorted(VOCAB, key=lambda x: x[1], reverse=True) ) }\n",
    "\n",
    "INDEX_TO_WORDS = {ii:word for word, ii in WORDS_TO_INDEX.items()}\n",
    "\n",
    "#print (WORDS_TO_INDEX)\n",
    "#print (INDEX_TO_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "    result_vec = np.zeros(dict_size)\n",
    "    for word in text.split():\n",
    "        if word in words_to_index:\n",
    "            result_vec[words_to_index[word]] +=1\n",
    "    return result_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtained vector: [1. 1. 0. 1.]\n",
      "correct ansswer: [1, 1, 0, 1]\n",
      "The two are equal (T/F): True\n"
     ]
    }
   ],
   "source": [
    "# test my bag of words\n",
    "mytext = ['hi how are you']\n",
    "words_to_index = {'hi': 0, 'you': 1, 'me': 2, 'are': 3} # these are the most common words already found\n",
    "ans = [1, 1, 0, 1]\n",
    "\n",
    "for i, text in enumerate(mytext):\n",
    "    vec = my_bag_of_words(text, words_to_index, 4)\n",
    "    print ('obtained vector:', vec)\n",
    "    print ('correct ansswer:', ans)\n",
    "    print ( 'The two are equal (T/F):',(vec==ans).any() )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape  (100000, 1000)\n",
      "X_val shape  (30000, 1000)\n",
      "X_test shape  (20000, 1000)\n"
     ]
    }
   ],
   "source": [
    "X_train_mybag = sp_sparse.vstack([ sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
    "X_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_val])\n",
    "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])\n",
    "print('X_train shape ', X_train_mybag.shape)\n",
    "print('X_val shape ', X_val_mybag.shape)\n",
    "print('X_test shape ', X_test_mybag.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "\n",
    "For the 11th row in X_train_mybag find how many non-zero elements it has. In this task the answer (variable non_zero_elements_count) should be a number, e.g. 20.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = X_train_mybag[10].toarray()[0]\n",
    "\n",
    "non_zero_elements_count = np.sum([1 for item in row if item != 0])\n",
    "print (non_zero_elements_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "    Implement function tfidf_features using class TfidfVectorizer from scikit-learn. Use train corpus to train a vectorizer. Don't forget to take a look into the arguments that you can pass to it. We suggest that you filter out too rare words (occur less than in 5 titles) and too frequent words (occur more than in 90% of the titles). Also, use bigrams along with unigrams in your vocabulary.\n",
    "    \n",
    "    First use TfidfVectorizer without token_pattern and see if you have 'c+' in tfidf_vocab if not then use '(\\S+)' regexp as a token_pattern in the constructor of the vectorizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_features(X_train, X_val, X_test):\n",
    "    # max_df and min_df is to filter out too frequent and too rare words\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 max_df=0.9, min_df=5,\n",
    "                                 token_pattern='(\\S+)' )\n",
    "    \n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    return (X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vectorizer.vocabulary_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_val, X_test)\n",
    "tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vocab['c#']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiLabel Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes=sorted(tags_counts.keys()))\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_val = mlb.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X_train, y_train):\n",
    "    model = OneVsRestClassifier(LogisticRegression(penalty='l2', C=1.0, max_iter=500))\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_mybag = train_classifier(X_train_mybag, y_train)\n",
    "classifier_tfidf = train_classifier(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\n",
    "y_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\n",
    "\n",
    "y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\n",
    "y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_inversed = mlb.inverse_transform(y_val_predicted_labels_tfidf)\n",
    "y_val_inversed = mlb.inverse_transform(y_val)\n",
    "for i in range(3):\n",
    "    print('Title:\\t{}\\nTrue labels:\\t{}\\nPredicted labels:\\t{}\\n\\n'.format(\n",
    "        X_val[i],\n",
    "        ','.join(y_val_inversed[i]),\n",
    "        ','.join(y_val_pred_inversed[i])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "To evaluate the results we will use several classification metrics:\n",
    " - [Accuracy](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    " - [F1-score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    " - [Area under ROC-curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    " - [Area under precision-recall curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) \n",
    " \n",
    "Make sure you are familiar with all of them. How would you expect the things work for the multi-label scenario? Read about micro/macro/weighted averaging following the sklearn links provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score   #  extra arguement 'average' is required for multiclass/multilabel targets.\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_scores(y_val, predicted):\n",
    "    print (\"Accracy={}\".format(accuracy_score(y_val, predicted)), \n",
    "        \"F1_macro={}\".format(f1_score(y_val, predicted, average='macro')),\n",
    "        \"F1_micro={}\".format(f1_score(y_val, predicted, average='micro')),\n",
    "        \"F1_wted={}\".format(f1_score(y_val, predicted, average='weighted')),\n",
    "        \"Precsion_macro={}\".format(average_precision_score(y_val, predicted, average='macro')),\n",
    "        \"Precsion_micro={}\".format(average_precision_score(y_val, predicted, average='micro')),\n",
    "        \"Precsion_wted={}\".format(average_precision_score(y_val, predicted, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bag-of-words')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_mybag)\n",
    "print('Tfidf')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utility_metrics import roc_auc\n",
    "#from utils.wk1_utility_metrics import roc_auc\n",
    "from utils_metrics import roc_auc\n",
    "n_classes = len(tags_counts)\n",
    "roc_auc(y_val, y_val_predicted_scores_mybag, n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(tags_counts)\n",
    "roc_auc(y_val, y_val_predicted_scores_tfidf, n_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultilabelClassification\n",
    "\n",
    "Once we have the evaluation set up, we suggest that you experiment a bit with training your classifiers. We will use F1-score weighted as an evaluation metric. Our recommendation:\n",
    "\n",
    "compare the quality of the bag-of-words and TF-IDF approaches and chose one of them.\n",
    "for the chosen one, try L1 and L2-regularization techniques in Logistic Regression with different coefficients (e.g. C equal to 0.1, 1, 10, 100).\n",
    "You also could try other improvements of the preprocessing / model, if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE #############\n",
    "test_predictions = classifier_tfidf.predict(X_test_tfidf) \n",
    "test_pred_inversed = mlb.inverse_transform(test_predictions)\n",
    "\n",
    "test_predictions_for_submission = '\\n'.join('%i\\t%s' % (i, ','.join(row)) for i, row in enumerate(test_pred_inversed))\n",
    "#grader.submit_tag('MultilabelClassification', test_predictions_for_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (test_predictions_for_submission[:100] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_words_for_tag(classifier, tag, tags_classes, index_to_words, all_words):\n",
    "    \"\"\"\n",
    "        classifier: trained classifier\n",
    "        tag: particular tag\n",
    "        tags_classes: a list of classes names from MultiLabelBinarizer\n",
    "        index_to_words: index_to_words transformation\n",
    "        all_words: all words in the dictionary\n",
    "        \n",
    "        return nothing, just print top 5 positive and top 5 negative words for current tag\n",
    "    \"\"\"\n",
    "    print('Tag:\\t{}'.format(tag))\n",
    "    \n",
    "    # Extract an estimator from the classifier for the given tag.\n",
    "    # Extract feature coefficients from the estimator. \n",
    "    estimator = classifier.estimators_[tags_classes.index(tag)]\n",
    "    coff = estimator.coef_[0]\n",
    "    coff_idx = list(enumerate(coff))\n",
    "    top_pos_words_idx = [idx for idx, wt in sorted(coff_idx, key=lambda x: x[1], reverse=True)[:5]]\n",
    "    top_neg_words_idx = [idx for idx, wt in sorted(coff_idx, key=lambda x: x[1], reverse=False)[:5]]\n",
    "    top_positive_words = [index_to_words[idx] for idx in top_pos_words_idx] # top-5 words sorted by the coefficiens.\n",
    "    top_negative_words = [index_to_words[idx] for idx in top_neg_words_idx] # bottom-5 words  sorted by the coefficients.\n",
    "    print('Top positive words:\\t{}'.format(', '.join(top_positive_words)))\n",
    "    print('Top negative words:\\t{}\\n'.format(', '.join(top_negative_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_words_for_tag(classifier_tfidf, 'c', mlb.classes, tfidf_reversed_vocab, ALL_WORDS)\n",
    "print_words_for_tag(classifier_tfidf, 'c++', mlb.classes, tfidf_reversed_vocab, ALL_WORDS)\n",
    "print_words_for_tag(classifier_tfidf, 'linux', mlb.classes, tfidf_reversed_vocab, ALL_WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
